{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/matteoferrazzi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/matteoferrazzi/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/matteoferrazzi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import glob, nltk\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import datetime as dt\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('vader_lexicon')\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "import functools\n",
    "from string import digits\n",
    "import yfinance as yf\n",
    "import requests\n",
    "import bs4 as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition of the functions we will use to clean the text. We used the ones implemented in exercise sessions\n",
    "def remove_urls (vTEXT):\n",
    "    vTEXT = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', vTEXT, flags=re.MULTILINE)\n",
    "    return(vTEXT)\n",
    "\n",
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"custom function to remove the punctuation\"\"\"\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "def remove_digits_lengthword(text):\n",
    "    return \" \".join([word for word in str(text).split() if len(word)<20 and len(word)>2 and not word.isdigit()])\n",
    "                     \n",
    "def lemmatize(text):\n",
    "    le = WordNetLemmatizer()\n",
    "    return \" \".join([le.lemmatize(word) for word in str(text).split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a table with some features of the s&p500 such as symbol, sector and name of each stock\n",
    "table=pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "df = table[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition of a function to download submissions and the selftext from reddit searching for a query \n",
    "#with and to clean them using the functions implemented above. If the symbol of the stock is something \n",
    "#that can be found in reddit but not referring at that stock\n",
    "def searchsymbol(df,subreddit,type):\n",
    "    clean_comments1 = []\n",
    "    date = []\n",
    "    sector = []\n",
    "    name = []\n",
    "    for i in range(len(df['Symbol'])):\n",
    "        print(df['Symbol'][i])\n",
    "        search_query = df['Symbol'][i]\n",
    "        if df['Symbol'][i] == 'MMM':\n",
    "            search_query = '3M'\n",
    "        if df['Symbol'][i] == 'T':\n",
    "            search_query = 'AT&T'\n",
    "        if df['Symbol'][i] == 'A':\n",
    "            search_query = 'Agilent'\n",
    "        if df['Symbol'][i] == 'C':\n",
    "            search_query = 'Citi'\n",
    "        if df['Symbol'][i] == 'D':\n",
    "            search_query = 'Dominion Energy'\n",
    "        if df['Symbol'][i] == 'F':\n",
    "            search_query = 'Ford'\n",
    "        if df['Symbol'][i] == 'J':\n",
    "            search_query = 'Jacobs'\n",
    "        if df['Symbol'][i] == 'K':\n",
    "            search_query = 'Kellogg\\'s'\n",
    "        if df['Symbol'][i] == 'L':\n",
    "            search_query = 'Loews'\n",
    "        if df['Symbol'][i] == 'V':\n",
    "            search_query = 'Visa'\n",
    "        if df['Symbol'][i] == 'O':\n",
    "            search_query = 'Realty Income'\n",
    "        if df['Symbol'][i] == 'IT':\n",
    "            search_query = 'Gartner'\n",
    "        if df['Symbol'][i] == 'ON':\n",
    "            search_query = 'ON Semiconductor'\n",
    "        if df['Symbol'][i] == 'TT':\n",
    "            search_query = 'Trane'\n",
    "        if df['Symbol'][i] == 'TECH':\n",
    "            search_query = 'Bio-Techne'\n",
    "        submissions = subreddit.search(search_query,sort=type,limit=None,time_filter='year')\n",
    "\n",
    "        for submission in submissions:\n",
    "            clean_text1 = submission.title+submission.selftext\n",
    "            date.append(dt.datetime.utcfromtimestamp(submission.created_utc).date())\n",
    "            clean_text1 = clean_text1.lower()\n",
    "            clean_text1 = remove_urls(clean_text1)\n",
    "            clean_text1 = remove_punctuation(clean_text1)\n",
    "            clean_text1 = remove_stopwords(clean_text1)\n",
    "            clean_text1 = remove_digits_lengthword(clean_text1)\n",
    "            clean_text1 = lemmatize(clean_text1)\n",
    "            clean_comments1.append(clean_text1)\n",
    "            sector.append(df['GICS Sector'][i])\n",
    "            name.append(df['Symbol'][i])\n",
    "    return clean_comments1,date,sector,name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same function as before but looking for the name of the stock a squery\n",
    "def searchsecurity(df,subreddit,type):\n",
    "    clean_comments1 = []\n",
    "    date = []\n",
    "    name = []\n",
    "    sector = []\n",
    "\n",
    "    for i in range(len(df['Security'])):\n",
    "        print(df['Security'][i])\n",
    "        search_query = df['Security'][i]\n",
    "\n",
    "        submissions = subreddit.search(search_query,sort=type,limit=None,time_filter='year')\n",
    "\n",
    "        # Loop through comments and clean the data\n",
    "        for submission in submissions:\n",
    "            # Clean the comment text by removing punctuation, numbers, and special characters\n",
    "            clean_text1 = submission.title+submission.selftext\n",
    "            date.append(dt.datetime.utcfromtimestamp(submission.created_utc).date())\n",
    "            clean_text1 = clean_text1.lower()\n",
    "            clean_text1 = remove_urls(clean_text1)\n",
    "            clean_text1 = remove_punctuation(clean_text1)\n",
    "            clean_text1 = remove_stopwords(clean_text1)\n",
    "            clean_text1 = remove_digits_lengthword(clean_text1)\n",
    "            clean_text1 = lemmatize(clean_text1)\n",
    "                \n",
    "            # Append the cleaned comment text to the list\n",
    "            clean_comments1.append(clean_text1)\n",
    "            sector.append(df['GICS Sector'][i])\n",
    "            name.append(df['Symbol'][i])\n",
    "    return clean_comments1,date,sector,name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Reddit instance to download \n",
    "reddit = praw.Reddit(client_id='GpASx1ZazRoHcNZNDdEIOQ',\n",
    "                     client_secret='TdxMpCPbbb3KTihf0WDXegjujgPxSg',\n",
    "                     user_agent='econ/1.0 by testamatta21')\n",
    "\n",
    "#\n",
    "subreddit = reddit.subreddit('WallStreetBets')\n",
    "\n",
    "#Search using the symbol:\n",
    "#Search by relevance submissions on the subreddit\n",
    "clean_commentsrel1,daterel1,namerel1,sectorrel1=searchsymbol(df,subreddit,'relevance')\n",
    "#Search by hot submissions on the subreddit\n",
    "clean_commentshot1,datehot1,namehot1,sectorhot1=searchsymbol(df,subreddit,'hot')\n",
    "#Search by top submissions on the subreddit\n",
    "clean_commentstop1,datetop1,nametop1,sectortop1=searchsymbol(df,subreddit,'top')\n",
    "#Search by new submissions on the subreddit\n",
    "clean_commentsnew1,datenew1,namenew1,sectornew1=searchsymbol(df,subreddit,'new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Search using the security:\n",
    "#Search by relevance submissions on the subreddit\n",
    "clean_commentsrel2,daterel2,namerel2,sectorrel2=searchsecurity(df,subreddit,'relevance')\n",
    "#Search by hot submissions on the subreddit\n",
    "clean_commentshot2,datehot2,namehot2,sectorhot2=searchsecurity(df,subreddit,'hot')\n",
    "#Search by top submissions on the subreddit\n",
    "clean_commentstop2,datetop2,nametop2,sectortop2=searchsecurity(df,subreddit,'top')\n",
    "#Search by new submissions on the subreddit\n",
    "clean_commentsnew2,datenew2,namenew2,sectornew2=searchsecurity(df,subreddit,'new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the results together\n",
    "clean_comments1=clean_commentsrel1+clean_commentshot1+clean_commentstop1+clean_commentsnew1+clean_commentsrel2+clean_commentshot2+clean_commentstop2+clean_commentsnew2\n",
    "sector=sectorrel1+sectorhot1+sectortop1+sectornew1+sectorrel2+sectorhot2+sectortop2+sectornew2\n",
    "date=daterel1+datehot1+datetop1+datenew1+daterel2+datehot2+datetop2+datenew2\n",
    "name=namerel1+namehot1+nametop1+namenew1+namerel2+namehot2+nametop2+namenew2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the dataframe removing duplicates and then save it as csv\n",
    "df1_old = pd.DataFrame({'comments': clean_comments1,'date':date,'sector':name,'name':sector})\n",
    "df1=df1_old.drop_duplicates()\n",
    "df1.to_csv('data1final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=pd.read_csv('data1final.csv')\n",
    "data1[\"comments\"] = data1['comments'].str.replace('[^\\w\\s]','')\n",
    "data1[list([type(data1.comments[i])==float for i in range(len(data1.comments))])]\n",
    "data1=data1.drop(data1[list([type(data1.comments[i])==float for i in range(len(data1.comments))])].index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
